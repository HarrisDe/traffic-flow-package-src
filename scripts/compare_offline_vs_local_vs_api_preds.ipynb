{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/tri_compare_predictions.py\n",
    "from __future__ import annotations\n",
    "import os, json, time, argparse, threading\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import joblib\n",
    "\n",
    "# Add the repo root (parent of `scripts/`) to sys.path\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parents[1]  # When run as .py\n",
    "except NameError:\n",
    "    ROOT = Path().resolve().parents[0]          # When run in Jupyter\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Import from your package (assumes `pip install -e .`)\n",
    "from traffic_flow.service.app import create_app\n",
    "from traffic_flow.service.runtime import InferenceRuntime\n",
    "from traffic_flow.pipeline.data_pipeline_orchestrator import TrafficDataPipelineOrchestrator\n",
    "from traffic_flow.inference.prediction_protocol import make_prediction_frame\n",
    "from traffic_flow.evaluation.model_comparison import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Utilities ----------------\n",
    "\n",
    "BAD_WEATHER_COLS = [\n",
    "    \"Snow_depth_surface\",\n",
    "    \"Water_equivalent_of_accumulated_snow_depth_surface\",\n",
    "]\n",
    "\n",
    "class ServerThread(threading.Thread):\n",
    "    \"\"\"Run Flask in-process so you don't need a second terminal.\"\"\"\n",
    "    def __init__(self, artifact_path: str, host=\"127.0.0.1\", port=8080):\n",
    "        super().__init__(daemon=True)\n",
    "        self.host = host; self.port = port\n",
    "        self.app = create_app(artifact_path=artifact_path)\n",
    "        from werkzeug.serving import make_server\n",
    "        self.srv = make_server(host, port, self.app)\n",
    "        self.ctx = self.app.app_context()\n",
    "        self.ctx.push()\n",
    "    def run(self): self.srv.serve_forever()\n",
    "    def shutdown(self):\n",
    "        self.srv.shutdown()\n",
    "        self.ctx.pop()\n",
    "\n",
    "def iter_batches(df: pd.DataFrame, batch_size: int) -> Iterable[pd.DataFrame]:\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size].copy()\n",
    "\n",
    "def to_json_records_strict(df: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"Use pandas to_json to convert NaN/Inf to null; then parse back to dicts.\"\"\"\n",
    "    return json.loads(df.to_json(orient=\"records\", date_format=\"iso\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches_list(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(data[i:i+batch_size])\n",
    "        print(f\"going from i = {i} to i+batch_size = {i+batch_size}\")\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Build test RAW rows with same cleaning params ----------------\n",
    "\n",
    "def load_artifact(artifact: str | Path) -> dict:\n",
    "    b = joblib.load(artifact)\n",
    "    return {\"bundle\": b, \"states\": b[\"states\"], \"horizon\": int(b.get(\"horizon\", 15))}\n",
    "\n",
    "def make_orchestrator_from_states(raw_path: str | Path, states: dict) -> TrafficDataPipelineOrchestrator:\n",
    "    clean = states[\"clean_state\"]\n",
    "    tdp = TrafficDataPipelineOrchestrator(file_path=str(raw_path), sensor_encoding_type=\"mean\")\n",
    "    tdp.prepare_base_features(\n",
    "        window_size=clean[\"smoothing_window\"],\n",
    "        filter_extreme_changes=True,\n",
    "        smooth_speeds=True,\n",
    "        relative_threshold=clean[\"relative_threshold\"],\n",
    "        use_median_instead_of_mean_smoothing=clean[\"use_median\"],\n",
    "    )\n",
    "    return tdp\n",
    "\n",
    "def get_raw_test(raw_path: str | Path, states: dict) -> pd.DataFrame:\n",
    "    tdp = make_orchestrator_from_states(raw_path, states)\n",
    "    raw = pd.read_parquet(raw_path)\n",
    "    raw = raw.drop(columns=[c for c in BAD_WEATHER_COLS if c in raw.columns])  # drop problematic cols\n",
    "    test = raw.loc[raw[\"date\"] >= tdp.first_test_timestamp].copy()\n",
    "    test.sort_values([\"date\",\"sensor_id\"], kind=\"mergesort\", inplace=True)\n",
    "    return test,tdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Three ways to predict ----------------\n",
    "\n",
    "def api_predict_canonical(base_url: str, raw_test: pd.DataFrame, batch_size=20000, timeout=300) -> pd.DataFrame:\n",
    "    outs = []\n",
    "    for i, batch in enumerate(iter_batches(raw_test, batch_size), start=1):\n",
    "        b = batch.copy()\n",
    "        # format datetimes to string\n",
    "        b[\"date\"] = pd.to_datetime(b[\"date\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # strict json (nulls etc.)\n",
    "        records = to_json_records_strict(b)\n",
    "        print(f\"Sending {len(records)} records\")\n",
    "        print(f\"records[0] {records[0]}\")  # Inspect 1st sample\n",
    "        r = requests.post(f\"{base_url}/predict\", json={\"records\": records}, timeout=timeout)\n",
    "        print(f\"r.status_code : {r.status_code}\")\n",
    "        print(f\"r.text{r.text}\")\n",
    "        r.raise_for_status()\n",
    "        part = pd.DataFrame(r.json()[\"predictions\"])\n",
    "        outs.append(part)\n",
    "        print(f\"[API] batch {i} -> {len(part)} preds\")\n",
    "    out = pd.concat(outs, ignore_index=True) if outs else pd.DataFrame()\n",
    "    # Normalize dtypes\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out = out.sort_values([\"prediction_time\", \"sensor_id\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def local_predict_canonical(artifact_path: str, raw_test: pd.DataFrame, batch_size=20000) -> pd.DataFrame:\n",
    "    rt = InferenceRuntime(artifact_path)\n",
    "    outs = []\n",
    "    for i, batch in enumerate(iter_batches(raw_test, batch_size), start=1):\n",
    "        pred_df, _ = rt.predict_df(batch)  # already canonical\n",
    "        outs.append(pred_df)\n",
    "        print(f\"[LOCAL] batch {i} -> {len(pred_df)} preds\")\n",
    "    out = pd.concat(outs, ignore_index=True) if outs else pd.DataFrame()\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out = out.sort_values([\"prediction_time\", \"sensor_id\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def evaluator_offline_canonical(artifact_path: str, raw_path: str | Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Rebuilds training-like X_test/df using the artifact's states, then uses\n",
    "    ModelEvaluator to produce canonical predictions for test rows.\n",
    "    Returns (pred_df, truth_lookup) where truth_lookup has:\n",
    "      sensor_id, prediction_time, y_act\n",
    "    \"\"\"\n",
    "    art = load_artifact(artifact_path)\n",
    "    states, horizon = art[\"states\"], art[\"horizon\"]\n",
    "\n",
    "    # 1) Rebuild training split & X/y using same params; keep df (full)\n",
    "    tdp = make_orchestrator_from_states(raw_path, states)\n",
    "    tdp.finalise_for_horizon(horizon=horizon, drop_datetime=True)  # keep 'date' visible\n",
    "    X_train, X_test, y_train, y_test = tdp.X_train, tdp.X_test, tdp.y_train, tdp.y_test\n",
    "    df_all = tdp.df\n",
    "\n",
    "    # 2) Evaluator + canonical preds (test only)\n",
    "    me = ModelEvaluator(\n",
    "        X_test=X_test,\n",
    "        df_for_ML=df_all,       # evaluator will internally take test_set\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        target_is_gman_error_prediction=False,\n",
    "        y_is_normalized=False,\n",
    "        rounding=6,\n",
    "    )\n",
    "    model = art[\"bundle\"][\"model\"]\n",
    "    pred_df = me.to_canonical_predictions(model=model, states=states, horizon_min=horizon)\n",
    "\n",
    "    # 3) Smoothed ground truth at prediction_time (training target)\n",
    "    truth = df_all.loc[df_all[\"test_set\"], [\"sensor_id\", \"date_of_prediction\", \"target_total_speed\"]].copy()\n",
    "    truth.rename(columns={\"date_of_prediction\": \"prediction_time\", \"target_total_speed\": \"y_act\"}, inplace=True)\n",
    "    truth[\"prediction_time\"] = pd.to_datetime(truth[\"prediction_time\"])\n",
    "    truth = truth.sort_values([\"prediction_time\",\"sensor_id\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return pred_df, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_steps(states: dict, default: int = 25) -> int:\n",
    "    # Adjust the key path to your states layout if necessary\n",
    "    return int(states.get(\"lag_state\", {}).get(\"lags\", default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Compare helpers ----------------\n",
    "\n",
    "def attach_y_act(pred_df: pd.DataFrame, truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    pred = pred_df.copy()\n",
    "    pred[\"prediction_time\"] = pd.to_datetime(pred[\"prediction_time\"])\n",
    "    out = pred.merge(truth, on=[\"sensor_id\", \"prediction_time\"], how=\"left\")\n",
    "    return out.loc[:, [\"prediction_time\", \"sensor_id\", \"y_act\", \"y_pred_total\"]].rename(columns={\"y_pred_total\": \"y_pred\"})\n",
    "\n",
    "def check_equal(a: pd.DataFrame, b: pd.DataFrame, tol=1e-8) -> Tuple[bool,float]:\n",
    "    key = [\"sensor_id\", \"prediction_time\"]\n",
    "    ma = a.copy(); mb = b.copy()\n",
    "    ma[\"prediction_time\"] = pd.to_datetime(ma[\"prediction_time\"])\n",
    "    mb[\"prediction_time\"] = pd.to_datetime(mb[\"prediction_time\"])\n",
    "    m = ma.merge(mb, on=key, suffixes=(\"_a\",\"_b\"), how=\"inner\")\n",
    "    if m.empty:\n",
    "        return False, float(\"inf\")\n",
    "    diff = (m[\"y_pred_a\"] - m[\"y_pred_b\"]).to_numpy()\n",
    "    return bool(np.max(np.abs(diff)) <= tol), float(np.max(np.abs(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Paths (adapt as needed)\n",
    "artifact = Path(\"../../artifacts/traffic_pipeline_h-15.joblib\")\n",
    "raw_path = Path(\"../../data/NDW/ndw_three_weeks.parquet\")\n",
    "url = \"http://127.0.0.1:8080\"\n",
    "start_server = True\n",
    "batch_size = 20000\n",
    "tolerance = 1e-6\n",
    "save_outputs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = None\n",
    "if start_server:\n",
    "    os.environ[\"ARTIFACT_PATH\"] = str(artifact)\n",
    "    host, port = \"127.0.0.1\", int(url.rsplit(\":\", 1)[-1])\n",
    "    server = ServerThread(artifact_path=str(artifact), host=host, port=port)\n",
    "    server.start()\n",
    "\n",
    "    for _ in range(60):\n",
    "        try:\n",
    "            if requests.get(f\"{url}/healthz\", timeout=1).ok:\n",
    "                print(\"Server is up!\")\n",
    "                break\n",
    "        except Exception:\n",
    "            time.sleep(0.25)\n",
    "    else:\n",
    "        raise RuntimeError(\"API failed to come up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_parquet('../../data/NDW/ndw_three_weeks.parquet')\n",
    "artifact_load = load_artifact(artifact)\n",
    "states = load_artifact(artifact)[\"states\"]\n",
    "\n",
    "# 1) Prepare RAW\n",
    "raw_test,tdp = get_raw_test(raw_path, states)\n",
    "raw = raw.loc[raw['date']>=tdp.first_test_timestamp]\n",
    "\n",
    "# 2) Evaluator (offline)\n",
    "eval_pred, truth = evaluator_offline_canonical(artifact, raw_path)\n",
    "df_eval = attach_y_act(eval_pred, truth)\n",
    "\n",
    "# 3) Local runtime\n",
    "df_local_pred = local_predict_canonical(str(artifact), raw_test, batch_size=batch_size)\n",
    "df_local = attach_y_act(df_local_pred, truth)\n",
    "\n",
    "# 4) API\n",
    "df_api_pred = api_predict_canonical(url, raw_test, batch_size=batch_size)\n",
    "df_api = attach_y_act(df_api_pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['lag_state']['lags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_load = load_artifact(artifact)\n",
    "artifact_load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_load['bundle'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare outputs\n",
    "ok_al, maxdiff_al = check_equal(df_api.rename(columns={\"y_pred\":\"y_pred_a\"}), df_local.rename(columns={\"y_pred\":\"y_pred_b\"}), tol=tolerance)\n",
    "ok_ae, maxdiff_ae = check_equal(df_api.rename(columns={\"y_pred\":\"y_pred_a\"}), df_eval.rename(columns={\"y_pred\":\"y_pred_b\"}), tol=tolerance)\n",
    "ok_le, maxdiff_le = check_equal(df_local.rename(columns={\"y_pred\":\"y_pred_a\"}), df_eval.rename(columns={\"y_pred\":\"y_pred_b\"}), tol=tolerance)\n",
    "\n",
    "print(\"\\n=== Parity checks (predictions only) ===\")\n",
    "print(f\"API vs LOCAL:  equal within tol={tolerance}? {ok_al}  (max|Δ|={maxdiff_al:.3g})\")\n",
    "print(f\"API vs EVAL :  equal within tol={tolerance}? {ok_ae}  (max|Δ|={maxdiff_ae:.3g})\")\n",
    "print(f\"LOCAL vs EVAL: equal within tol={tolerance}? {ok_le}  (max|Δ|={maxdiff_le:.3g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view/save results\n",
    "print(\"API head:\\n\", df_api.head())\n",
    "print(\"LOCAL head:\\n\", df_local.head())\n",
    "print(\"EVAL head:\\n\", df_eval.head())\n",
    "\n",
    "if save_outputs:\n",
    "    Path(\"outputs\").mkdir(exist_ok=True)\n",
    "    df_api.to_csv(\"outputs/api_with_y_act.csv\", index=False)\n",
    "    df_local.to_csv(\"outputs/local_with_y_act.csv\", index=False)\n",
    "    df_eval.to_csv(\"outputs/eval_with_y_act.csv\", index=False)\n",
    "    print(\"Saved CSVs to outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown server\n",
    "if server:\n",
    "    print(\"Shutting down server...\")\n",
    "    server.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harris_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
