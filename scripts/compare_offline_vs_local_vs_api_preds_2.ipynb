{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Notebook-friendly: API vs Local (with smoothed y_act) ===================\n",
    "# Paste this cell into a Jupyter notebook.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time, math, threading\n",
    "from typing import Iterable, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Import your package (assumes `pip install -e .` from repo root)\n",
    "# If needed, adjust ROOT so imports resolve in your environment.\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    # Try to infer repo root two levels up from the notebook location\n",
    "    ROOT = Path.cwd()\n",
    "    while ROOT.name not in (\"traffic_flow_package_src\", \"\") and ROOT.parent != ROOT:\n",
    "        ROOT = ROOT.parent\n",
    "    if str(ROOT) not in os.sys.path:\n",
    "        os.sys.path.insert(0, str(ROOT))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from traffic_flow.service.app import create_app\n",
    "from traffic_flow.service.runtime import InferenceRuntime\n",
    "from traffic_flow.pipeline.data_pipeline_orchestrator import TrafficDataPipelineOrchestrator\n",
    "from traffic_flow.evaluation.model_comparison import ModelEvaluator  # only to reuse some helpers if needed\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Config you can tweak in the notebook right after executing this cell\n",
    "# -------------------------------------------------------------------\n",
    "ARTIFACT   = Path(\"../../artifacts/traffic_pipeline_h-15.joblib\")\n",
    "RAW_PATH   = Path(\"../../data/NDW/ndw_three_weeks.parquet\")\n",
    "BASE_URL   = \"http://127.0.0.1:8080\"\n",
    "PORT       = 8080\n",
    "BATCH_ROWS = 20_000               # per-sensor chunk size\n",
    "TOL        = 1e-6                 # equality tolerance (abs max diff)\n",
    "BAD_WEATHER_COLS = [\n",
    "    \"Snow_depth_surface\",\n",
    "    \"Water_equivalent_of_accumulated_snow_depth_surface\",\n",
    "]\n",
    "\n",
    "\n",
    "# ================== Server runner (in-process) =====================\n",
    "\n",
    "class ServerThread(threading.Thread):\n",
    "    \"\"\"Run the Flask app in-process so you don't need a second terminal.\"\"\"\n",
    "    def __init__(self, artifact_path: str, host=\"127.0.0.1\", port=8080):\n",
    "        super().__init__(daemon=True)\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.app = create_app(artifact_path=artifact_path)\n",
    "        from werkzeug.serving import make_server\n",
    "        self._srv = make_server(self.host, self.port, self.app)\n",
    "        self._ctx = self.app.app_context()\n",
    "        self._ctx.push()\n",
    "    def run(self): self._srv.serve_forever()\n",
    "    def shutdown(self):\n",
    "        self._srv.shutdown()\n",
    "        self._ctx.pop()\n",
    "\n",
    "\n",
    "# ================= Helpers: states, cleaning, truth =================\n",
    "\n",
    "def load_artifact(artifact_path: str | Path) -> dict:\n",
    "    b = joblib.load(artifact_path)\n",
    "    return {\"bundle\": b, \"states\": b[\"states\"], \"horizon\": int(b.get(\"horizon\", 15))}\n",
    "\n",
    "def get_lag_steps(states: dict, default: int = 25) -> int:\n",
    "    # Your orchestrator saves TemporalLagFeatureAdder under \"lag_state\"\n",
    "    # Most likely the state has key \"lags\" (int). Adjust if your key differs.\n",
    "    return int(states.get(\"lag_state\", {}).get(\"lags\", default))\n",
    "\n",
    "def make_orchestrator_from_states(raw_path: str | Path, states: dict) -> TrafficDataPipelineOrchestrator:\n",
    "    \"\"\"Use the artifact's cleaning params so train/test split & smoothing match training.\"\"\"\n",
    "    clean = states[\"clean_state\"]\n",
    "    tdp = TrafficDataPipelineOrchestrator(file_path=str(raw_path), sensor_encoding_type=\"mean\")\n",
    "    tdp.prepare_base_features(\n",
    "        window_size=clean[\"smoothing_window\"],\n",
    "        filter_extreme_changes=True,\n",
    "        smooth_speeds=True,\n",
    "        relative_threshold=clean[\"relative_threshold\"],\n",
    "        use_median_instead_of_mean_smoothing=clean[\"use_median\"],\n",
    "    )\n",
    "    return tdp\n",
    "\n",
    "def get_raw_test(raw_path: str | Path, states: dict) -> pd.DataFrame:\n",
    "    tdp = make_orchestrator_from_states(raw_path, states)\n",
    "    raw = pd.read_parquet(raw_path)\n",
    "    # (Optional) drop problematic JSON cols / reduce payload size\n",
    "    raw = raw.drop(columns=[c for c in BAD_WEATHER_COLS if c in raw.columns], errors=\"ignore\")\n",
    "    test = raw.loc[raw[\"date\"] >= tdp.first_test_timestamp].copy()\n",
    "    test.sort_values([\"sensor_id\", \"date\"], kind=\"mergesort\", inplace=True)\n",
    "    return test\n",
    "\n",
    "def build_smoothed_y_act(raw_path: str | Path, states: dict, horizon: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rebuild training-style target on test rows:\n",
    "      y_act = 'target_total_speed' at 'date_of_prediction'\n",
    "    \"\"\"\n",
    "    tdp = make_orchestrator_from_states(raw_path, states)\n",
    "    tdp.finalise_for_horizon(horizon=horizon, drop_datetime=False)  # keep date cols visible\n",
    "    df = tdp.df.loc[tdp.df[\"test_set\"], [\"sensor_id\", \"date_of_prediction\", \"target_total_speed\"]].copy()\n",
    "    df.rename(columns={\"date_of_prediction\": \"prediction_time\", \"target_total_speed\": \"y_act\"}, inplace=True)\n",
    "    df[\"prediction_time\"] = pd.to_datetime(df[\"prediction_time\"])\n",
    "    df.sort_values([\"sensor_id\", \"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========== Overlapped per-sensor batching (preserves lags) ==========\n",
    "\n",
    "def api_predict_sensor_overlapped(base_url: str,\n",
    "                                  df_sensor: pd.DataFrame,\n",
    "                                  lag_steps: int,\n",
    "                                  chunk_rows: int = 20_000,\n",
    "                                  timeout: int = 300) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Send one sensor's rows in chunks with an overlap of lag_steps rows\n",
    "    so lag features are computed with correct history.\n",
    "    \"\"\"\n",
    "    df_sensor = df_sensor.sort_values(\"date\", kind=\"mergesort\").reset_index(drop=True)\n",
    "    out = []\n",
    "    n = len(df_sensor)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end   = min(start + chunk_rows, n)\n",
    "        warm  = max(0, start - lag_steps)\n",
    "        chunk = df_sensor.iloc[warm:end].copy()\n",
    "        # datetime -> string for JSON\n",
    "        chunk[\"date\"] = pd.to_datetime(chunk[\"date\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        records = json.loads(chunk.to_json(orient=\"records\", date_format=\"iso\"))\n",
    "        r = requests.post(f\"{base_url}/predict\", json={\"records\": records}, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        pred = pd.DataFrame(r.json()[\"predictions\"])\n",
    "        pred[\"input_time\"] = pd.to_datetime(pred[\"input_time\"])\n",
    "\n",
    "        # Drop the warm-up predictions from this batch\n",
    "        keep_from = pd.to_datetime(df_sensor.loc[start, \"date\"])\n",
    "        pred = pred.loc[pred[\"input_time\"] >= keep_from].copy()\n",
    "        out.append(pred)\n",
    "        start += chunk_rows\n",
    "\n",
    "    if not out:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(out, ignore_index=True)\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out.sort_values([\"sensor_id\",\"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "def local_predict_sensor_overlapped(rt: InferenceRuntime,\n",
    "                                    df_sensor: pd.DataFrame,\n",
    "                                    lag_steps: int,\n",
    "                                    chunk_rows: int = 20_000) -> pd.DataFrame:\n",
    "    df_sensor = df_sensor.sort_values(\"date\", kind=\"mergesort\").reset_index(drop=True)\n",
    "    out = []\n",
    "    n = len(df_sensor)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end   = min(start + chunk_rows, n)\n",
    "        warm  = max(0, start - lag_steps)\n",
    "        chunk = df_sensor.iloc[warm:end].copy()\n",
    "        pred_df, _ = rt.predict_df(chunk)\n",
    "        pred_df[\"input_time\"] = pd.to_datetime(pred_df[\"input_time\"])\n",
    "\n",
    "        keep_from = pd.to_datetime(df_sensor.loc[start, \"date\"])\n",
    "        pred_df = pred_df.loc[pred_df[\"input_time\"] >= keep_from].copy()\n",
    "        out.append(pred_df)\n",
    "        start += chunk_rows\n",
    "\n",
    "    if not out:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(out, ignore_index=True)\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out.sort_values([\"sensor_id\",\"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "def api_predict_all_sensors(base_url: str,\n",
    "                            raw_test: pd.DataFrame,\n",
    "                            states: dict,\n",
    "                            chunk_rows: int = 20_000,\n",
    "                            timeout: int = 300) -> pd.DataFrame:\n",
    "    L = get_lag_steps(states)\n",
    "    outs = []\n",
    "    for sid, df_s in raw_test.groupby(\"sensor_id\", sort=False):\n",
    "        outs.append(api_predict_sensor_overlapped(base_url, df_s, L, chunk_rows, timeout))\n",
    "    out = pd.concat(outs, ignore_index=True) if outs else pd.DataFrame()\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out.sort_values([\"sensor_id\",\"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "def local_predict_all_sensors(artifact_path: str,\n",
    "                              raw_test: pd.DataFrame,\n",
    "                              states: dict,\n",
    "                              chunk_rows: int = 20_000) -> pd.DataFrame:\n",
    "    L = get_lag_steps(states)\n",
    "    rt = InferenceRuntime(str(artifact_path))\n",
    "    outs = []\n",
    "    for sid, df_s in raw_test.groupby(\"sensor_id\", sort=False):\n",
    "        outs.append(local_predict_sensor_overlapped(rt, df_s, L, chunk_rows))\n",
    "    out = pd.concat(outs, ignore_index=True) if outs else pd.DataFrame()\n",
    "    out[\"prediction_time\"] = pd.to_datetime(out[\"prediction_time\"])\n",
    "    out.sort_values([\"sensor_id\",\"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== Ground-truth & compare =======================\n",
    "\n",
    "def attach_y_act(pred_df: pd.DataFrame, truth_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pred = pred_df.copy()\n",
    "    pred[\"prediction_time\"] = pd.to_datetime(pred[\"prediction_time\"])\n",
    "    out = pred.merge(truth_df, on=[\"sensor_id\",\"prediction_time\"], how=\"left\")\n",
    "    out = out[[\"prediction_time\",\"sensor_id\",\"y_act\",\"y_pred_total\"]].rename(columns={\"y_pred_total\":\"y_pred\"})\n",
    "    out.sort_values([\"sensor_id\",\"prediction_time\"], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "def check_equal(a: pd.DataFrame, b: pd.DataFrame, tol: float = 1e-6) -> Tuple[bool,float]:\n",
    "    key = [\"sensor_id\",\"prediction_time\"]\n",
    "    A = a[key+[\"y_pred\"]].rename(columns={\"y_pred\":\"y_pred_a\"})\n",
    "    B = b[key+[\"y_pred\"]].rename(columns={\"y_pred\":\"y_pred_b\"})\n",
    "    m = A.merge(B, on=key, how=\"inner\")\n",
    "    if m.empty:\n",
    "        return False, float(\"inf\")\n",
    "    diff = (m[\"y_pred_a\"] - m[\"y_pred_b\"]).to_numpy()\n",
    "    return bool(np.max(np.abs(diff)) <= tol), float(np.max(np.abs(diff)))\n",
    "\n",
    "\n",
    "# =========================== Driver =================================\n",
    "\n",
    "def run_all(artifact: str | Path = ARTIFACT,\n",
    "            raw_path: str | Path = RAW_PATH,\n",
    "            url: str = BASE_URL,\n",
    "            start_server: bool = True,\n",
    "            batch_rows: int = BATCH_ROWS,\n",
    "            for_sensor: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      api_df, local_df, truth_df  (each canonical: prediction_time, sensor_id, y_act/y_pred)\n",
    "    \"\"\"\n",
    "    # 0) Load states & horizon\n",
    "    art = load_artifact(artifact)\n",
    "    states, horizon = art[\"states\"], art[\"horizon\"]\n",
    "\n",
    "    # 1) Optional: start API here\n",
    "    server = None\n",
    "    if start_server:\n",
    "        os.environ[\"ARTIFACT_PATH\"] = str(artifact)\n",
    "        host, port = \"127.0.0.1\", int(url.rsplit(\":\", 1)[-1])\n",
    "        server = ServerThread(artifact_path=str(artifact), host=host, port=port)\n",
    "        server.start()\n",
    "        # Wait for /healthz\n",
    "        for _ in range(60):\n",
    "            try:\n",
    "                if requests.get(f\"{url}/healthz\", timeout=1.0).ok:\n",
    "                    break\n",
    "            except Exception:\n",
    "                time.sleep(0.25)\n",
    "        else:\n",
    "            if server:\n",
    "                server.shutdown()\n",
    "            raise RuntimeError(\"Service did not become healthy on /healthz\")\n",
    "\n",
    "    try:\n",
    "        # 2) RAW test rows (aligned with training cleaning)\n",
    "        raw_test = get_raw_test(raw_path, states)\n",
    "        if for_sensor is not None:\n",
    "            raw_test = raw_test.loc[raw_test[\"sensor_id\"] == for_sensor].copy()\n",
    "            if raw_test.empty:\n",
    "                raise ValueError(f\"Sensor_id '{for_sensor}' not found in test split.\")\n",
    "\n",
    "        # 3) Build smoothed ground-truth (training target)\n",
    "        truth_df = build_smoothed_y_act(raw_path, states, horizon)\n",
    "\n",
    "        # 4) Predictions\n",
    "        api_pred  = api_predict_all_sensors(url, raw_test, states, chunk_rows=batch_rows)\n",
    "        local_pred= local_predict_all_sensors(artifact, raw_test, states, chunk_rows=batch_rows)\n",
    "\n",
    "        # 5) Attach y_act for plotting / analysis\n",
    "        api_df   = attach_y_act(api_pred,   truth_df)\n",
    "        local_df = attach_y_act(local_pred, truth_df)\n",
    "\n",
    "        # 6) Equality check\n",
    "        ok, max_abs = check_equal(api_df, local_df, tol=TOL)\n",
    "        print(f\"API vs Local equal within tol {TOL}: {ok}  |  max|Δ| = {max_abs:.3g}\")\n",
    "        return api_df, local_df, truth_df\n",
    "\n",
    "    finally:\n",
    "        if start_server and server:\n",
    "            server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prepare_base_features!!!!!!!!!!!!!!!!\n",
      "[MeanSensorEncoder] Mean encoding learned for 204 sensors. Global mean=93.85.\n",
      "[AdjacentSensorFeatureAdder] Adding adjacent sensor features.\n",
      "[AdjacentSensorFeatureAdder] Added features: downstream_sensor_1, upstream_sensor_1\n",
      "Running prepare_base_features!!!!!!!!!!!!!!!!\n",
      "[MeanSensorEncoder] Mean encoding learned for 204 sensors. Global mean=93.85.\n",
      "[AdjacentSensorFeatureAdder] Adding adjacent sensor features.\n",
      "[AdjacentSensorFeatureAdder] Added features: downstream_sensor_1, upstream_sensor_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisderalas/Documents/Emeralds/Emeralds_traffic_state_forecasting_repos/traffic_flow_project/traffic_flow_package_src/traffic_flow/pipeline/data_pipeline_orchestrator.py:291: UserWarning: [finalize_for_horizon] drop_datetime feature value is: False,                          drop_sensor_id value is: True. These columns both                               must not exist when training the model, either drop them before training or                                   rerun TrafficDataPipelineOrchestrator with both set to True (their default values).\n",
      "  warnings.warn(f\"[finalize_for_horizon] drop_datetime feature value is: {drop_datetime},\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PreviousWeekdayWindowFeatureEngineer] horizon=15′  window=[-0,+0]′ step=1′  aggs=-  mode=local\n",
      "[WeatherFeatureDropper] Will drop ['incremental_id', 'Per_cent_frozen_precipitation_surface', 'Precipitable_water_entire_atmosphere_single_layer', 'Precipitation_rate_surface_3_Hour_Average', 'Storm_relative_helicity_height_above_ground_layer', 'Total_precipitation_surface_3_Hour_Accumulation', 'Categorical_Rain_surface_3_Hour_Average', 'Categorical_Freezing_Rain_surface_3_Hour_Average', 'Categorical_Ice_Pellets_surface_3_Hour_Average', 'Categorical_Snow_surface_3_Hour_Average', 'Convective_Precipitation_Rate_surface_3_Hour_Average', 'Convective_precipitation_surface_3_Hour_Accumulation', 'U-Component_Storm_Motion_height_above_ground_layer', 'V-Component_Storm_Motion_height_above_ground_layer', 'Geopotential_height_highest_tropospheric_freezing', 'Relative_humidity_highest_tropospheric_freezing', 'Ice_cover_surface', 'Snow_depth_surface', 'Water_equivalent_of_accumulated_snow_depth_surface', 'Wind_speed_gust_surface', 'u-component_of_wind_maximum_wind', 'u-component_of_wind_height_above_ground', 'v-component_of_wind_maximum_wind', 'v-component_of_wind_height_above_ground']\n",
      "[TargetVariableCreator] Creating target variables.\n",
      "[TargetVariableCreator] Computed 'target_total_speed' and 'target_speed_delta'.\n",
      "[TargetVariableCreator] Using delta speed as final target.\n",
      "[TargetVariableCreator] Final target column ready. 6165900 rows retained after dropping NaNs.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:8080/predict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# e.g., \"RWS01_MONIBAS_0041hrr0592ra\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m BASE_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:8080\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m api_df, local_df, truth_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mARTIFACT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRAW_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_server\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# <— reuse existing server\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfor_sensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m api_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[1], line 283\u001b[0m, in \u001b[0;36mrun_all\u001b[0;34m(artifact, raw_path, url, start_server, batch_rows, for_sensor)\u001b[0m\n\u001b[1;32m    280\u001b[0m truth_df \u001b[38;5;241m=\u001b[39m build_smoothed_y_act(raw_path, states, horizon)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# 4) Predictions\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m api_pred  \u001b[38;5;241m=\u001b[39m \u001b[43mapi_predict_all_sensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m local_pred\u001b[38;5;241m=\u001b[39m local_predict_all_sensors(artifact, raw_test, states, chunk_rows\u001b[38;5;241m=\u001b[39mbatch_rows)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# 5) Attach y_act for plotting / analysis\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mapi_predict_all_sensors\u001b[0;34m(base_url, raw_test, states, chunk_rows, timeout)\u001b[0m\n\u001b[1;32m    189\u001b[0m outs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sid, df_s \u001b[38;5;129;01min\u001b[39;00m raw_test\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensor_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 191\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mapi_predict_sensor_overlapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    192\u001b[0m out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(outs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m    193\u001b[0m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m, in \u001b[0;36mapi_predict_sensor_overlapped\u001b[0;34m(base_url, df_sensor, lag_steps, chunk_rows, timeout)\u001b[0m\n\u001b[1;32m    135\u001b[0m records \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(chunk\u001b[38;5;241m.\u001b[39mto_json(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, date_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    136\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m: records}, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m--> 137\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(r\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    139\u001b[0m pred[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(pred[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/harris_py310/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:8080/predict"
     ]
    }
   ],
   "source": [
    "# Choose a sensor (or leave None for all sensors)\n",
    "sensor = None  # e.g., \"RWS01_MONIBAS_0041hrr0592ra\"\n",
    "BASE_URL = \"http://127.0.0.1:8080\"\n",
    "api_df, local_df, truth_df = run_all(\n",
    "    artifact=ARTIFACT,\n",
    "    raw_path=RAW_PATH,\n",
    "    url=BASE_URL,\n",
    "    start_server=False,   # <— reuse existing server\n",
    "    batch_rows=20_000,\n",
    "    for_sensor=None,\n",
    ")\n",
    "\n",
    "api_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harris_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
